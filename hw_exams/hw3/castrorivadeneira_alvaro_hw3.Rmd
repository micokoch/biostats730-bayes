---
title: "Applied Bayesian modeling - HW3"
author: "Álvaro J. Castro Rivadeneira"
date: "October 5, 2022"
header-includes:
    - \usepackage{bm}
output:
  pdf_document: default
  html_document:
    df_print: paged
---

Score: The maximum number of points in this HW is 20 points, with 3 points extra credit. This HW counts for 1.5 HWs, so points are rescaled such that a "full pass" score is 150%. Specifically, for calculating your final HW score, the points will be rescaled to a maximum score of (20+3)/20*150% = 172.5%. 


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(brms)
library(lme4)
library(broom.mixed)
library(tidybayes)
```

# Exercise 1: Fit a Bayesian model using brm and check and interpret the output (4 points)

We continue with IQ data, as introduced in HW2. For this HW, data set iq_scores.csv contains 10 iq-scores, sampled from a town in let's say, your favorite country. 

```{r}
iqs <- read_csv("iq_scores.csv", show_col_types = FALSE) %>% 
  mutate(y = iq_score)
# Quick look at data
head(iqs)
summary(iqs)
# Use a linear regression for estimates to compare with Bayesian estimates
iq.lm.fit <- lm(iq_score ~ 1, data = iqs)
summary(iq.lm.fit)
(iq.lm.mean.confint <- confint(iq.lm.fit, level = 0.8))
(iq.lm.sd <- sigma(iq.lm.fit))
# To calculate confidence intervals
n <- length(resid(iq.lm.fit))
k <- 2
alpha <- 0.2
(sigma <- summary(iq.lm.fit)$sigma)
(iq.lm.sd.low <- sqrt((n-(k+1))*sigma^2/qchisq(alpha/2, df = n-(k+1), lower.tail = FALSE)))
(iq.lm.sd.high <- sqrt((n-(k+1))*sigma^2/qchisq(1-alpha/2, df = n-(k+1), lower.tail = FALSE)))
```

Use brm to fit the following model to the IQ data:
\begin{eqnarray*}
y_i|\theta_i, \sigma^2 &\sim& N(\theta_i, \sigma^2) \text{(independent), for } i = 1, 2, \hdots, n; \\
\mu &\sim& N(100, 15^2);\\
\sigma &\sim& \text{use the brm-default}.
\end{eqnarray*}  

## Bayesian regression 
```{r, message  = FALSE}
mu_prior <- set_prior("normal(100,15)", class = "Intercept")

iq.fit <- brm(iq_score ~ 1, data = iqs, 
              prior = c(mu_prior), 
              file = "output/exercise1", 
              chains = 4, iter = 1000, warmup = 500, 
              cores = getOption("mc.cores", 4))
```

Quick summary overview
```{r}
summary(iq.fit) 
```

Then answer the following questions:  
(i) Plot a histogram of the posterior samples of $\mu$ and report a posterior point estimate and 80% CI for $\mu$.  
```{r}
# posterior_samples(iq.fit, pars = c("b_Intercept")) %>% 
#   ggplot(aes(x = b_Intercept)) + 
#   geom_histogram(bins=30) + 
#   theme_bw()

post.iq.mean <- as_draws_df(iq.fit, variable = c("b_Intercept"))
post.iq.mean %>% 
  ggplot(aes(x = b_Intercept)) + 
  geom_histogram(bins=30, fill = "#69b3a2")

(post.iq.mean.summ <- posterior_summary(iq.fit, probs = c(.1, .9), variable = "b_Intercept"))
```
**Answer**  
Posterior point estimate: 110.19, and the 80% CI for $\mu$ is 104.77 - 115.59. This is close to what was obtained with a simple linear model, and is slightly lower since the prior mean was also slightly lower than the data mean.  

(ii) Plot a histogram of the posterior samples of $\sigma$ and report a posterior point estimate and 80% CI for $\sigma$.  
```{r}
# posterior_samples(iq.fit, pars = c("sigma")) %>% 
#   ggplot(aes(x = sigma)) + 
#   geom_histogram(bins = 30) + 
#   theme_bw()

post.iq.sigma <- as_draws_df(iq.fit, variable = c("sigma"))
post.iq.sigma %>% 
  ggplot(aes(x = sigma)) + 
  geom_histogram(bins=30, fill = "#69b3a2")

(post.iq.sig.summ <- posterior_summary(iq.fit, probs = c(.1, .9), variable = "sigma"))
```
**Answer**  
Posterior point estimate: 14.25, and the 80% CI for $\sigma$ is 10.50 - 19.12. This is close to what was obtained with a simple linear model, where the estimate and 80% CI are: 13.13 (10.02 - 20.64).  

(iii) Can you report a posterior point estimate and 80% CI for $\mu/\sigma$? If yes, do so. If not, why not?   
**Answer**  
Yes. I can calculate the ratio from my posterior samples as follows:
```{r}
post.iq.meandivsigma <- post.iq.mean$b_Intercept/post.iq.sigma$sigma
(post.iq.meandivsig.summ <- posterior_summary(post.iq.meandivsigma, probs = c(.1, .9)))
# Check that it is approximately correct by getting ratio from previously calculated point estimates
post.iq.mean.summ[1]/post.iq.sig.summ[1]
# It's close, which gives me confidence that it was done correctly
```
The reason it is possible is that when I produce my model fit, I'm producing samples that include $\mu$ and $\sigma$, so that I can calculate the ratio of each of these samples and then calculate a point estimate and credible intervals based on their distribution.  

Note that the prior for $\mu$ can be specified with an additional argument in brm (as illustrated in optional material in module 5), as follows:
```{r}
#mu_prior <- set_prior("normal(100,15)", class = "Intercept")
#fit  <- brm(y ~ 1, prior = c(mu_prior), 
#           your other usual arguments)
```


# Exercise 2: Compare and contrast the MCMC diagnostics of two different model fits (4 points)  

Continue with the IQ data from Q1 (with y= IQ scores)  to fit the model  as specified below. Present and briefly summarize resulting MCMC diagnostics for $\mu$ (traceplots, Rhat, effective sample sizes). Then and comment on whether this model fit can be used for summarizing information regarding $\mu$. If not, why not? 


```{r}
iq.fit_bad <- brm(iq_score ~ 1, data = iqs, file = "output/exercise2", 
                  chains = 4, iter = 200, cores = getOption("mc.cores", 4),
                  control = list(adapt_delta = 0.6, max_treedepth = 4)
                  # these are NOT recommended options, trying to create problems here!
)
```

**MCMC diagnostics for $\mu$**  
```{r}
# plot(iq.fit, variable = c("b_Intercept"))
plot(iq.fit_bad, variable = c("b_Intercept"))
```
In this plot we see that the estimate for $\mu$ is first of all, wrong - it is much lower than our prior estimate. Moreover, looking at the distribution on the left, the data do not seem to be normally distributed. More importantly, looking at the plot on the left, we don't see the chains converging, and in fact chain 4 seems to diverge from the others. Further, there is no mixing, and the autocorrelation in the sampled values is high.  

```{r}
# summary(iq.fit)
summary(iq.fit_bad)
```
From these results, it is first obvious that they differ significantly from what we obtained in the previous exercise as well as what we found with a linear model. Further, we notice that Rhat is 2.36, which is much, much higher than the target value where $\hat R < 1.05$. Additionally, the ESS should be at least 400 (100 per Markov Chain), and in fact it is 5 for the Bulk-ESS and 19 for the Tail-ESS.  

Ultimately this means that this model fit can _not_ be used for summarizing information regarding $\mu$ because it does not adequately converge in a way that provides meaningful results. The chains did not mix well, and have a very low precision as judged by $S_{eff}$.  

# Exercise 3: (8 points)  

Background: The dataset "marriage.csv" contains (simulated) data on the age of first marriage for a number of women in Kenya. Information on the ethnic group is provided as well. For the questions below, let $y_i$ denote the age of first marriage for the $i$-th woman, and $j[i]$ her ethnic group. The goal is to learn more about the mean age of marriage within ethnic groups, using a Bayesian hierarchical model.  

For starters, I will look at the data:
```{r}
matri <- read_csv("marriage.csv", show_col_types = FALSE) %>% 
  mutate(y = agemarried, 
         j = ethnicgroup)
# Quick look at data
head(matri)
summary(matri)
# Create summary dataset with info for each ethnic group:
matrigrup <- matri %>% 
  group_by(ethnicgroup) %>% 
  summarise(npers = n(), ybar = mean(y))
matrigrup
summary(matrigrup)
# population mean
ybarbar <- mean(matri$y) 
matrigrup %>% 
  ggplot(aes(x = npers, y = ybar)) + 
  geom_point() + 
  geom_hline(mapping = aes(yintercept = ybarbar)) + 
  theme_bw()
```

I decided to use multilevel linear modeling for estimates to compare with Bayesian estimates
```{r, warning=FALSE}
# First I will make a simple linear model without coefficients to compare with the hierarchical model
matrisimple.lm.fit <- lm(agemarried ~ 1, data=matri)
summary(matrisimple.lm.fit)
confint(matrisimple.lm.fit)
# Now the hierarchical model
matri.lm.fit <- lmer(agemarried ~ (1 | ethnicgroup), data=matri)
summary(matri.lm.fit)
(matri.lm.mean <- fixef(matri.lm.fit))
(matri.lm.mean.confint <- confint(matri.lm.fit, level = 0.95)[3,])
# eta = alpha - mu_alpha
eta.lm.1 <- as_tibble(ranef(matri.lm.fit)$ethnicgroup, rownames = "ethnicgroup")
head(eta.lm.1)
eta.lm.2 <- as.data.frame(ranef(matri.lm.fit)) %>% 
  transform(lwr = condval - 1.96*condsd, upr = condval + 1.96*condsd)
head(eta.lm.2)
eta.lm <- broom.mixed::tidy(matri.lm.fit, effects = "ran_vals", conf.int = TRUE)
# alphas
lm.alphas <- coef(matri.lm.fit, summary = T)[1]$ethnicgroup %>% 
    as_tibble(rownames = "ethnicgroup") %>% 
    rename(alph = "(Intercept)") %>% 
    transform(ethnicgroup = as.integer(ethnicgroup))
head(lm.alphas)
lm.alphas.full <- eta.lm %>% 
    rename(ethnicgroup = level) %>% 
    transform(ethnicgroup = as.integer(ethnicgroup)) %>% 
    left_join(lm.alphas, by = "ethnicgroup") %>% 
    select(3,5:9) %>% 
    mutate(estimate = (matri.lm.mean + estimate), 
           conf.low = (conf.low + matri.lm.mean), 
           conf.high = (conf.high + matri.lm.mean))
head(lm.alphas.full)
# Make the plot of alpha ~ ybar
matrigruplmalpha <- lm.alphas %>% 
  left_join(matrigrup, by = "ethnicgroup")
matrigruplmalpha %>% 
  ggplot(aes(y = alph, x = ybar, size = npers)) + 
  geom_point() + 
  geom_abline(slope = 1, intercept = 0)
# Plot of alpha - ybar
matrigruplmalpha %>% 
  ggplot(aes(y = alph - ybar, x = npers)) +
  geom_point() +
  geom_hline(yintercept = 0)
```

To answer the questions below, fit the following Bayesian hierarchical model to the marriage data:  
$$y_i|\alpha_{j[i]},\sigma_y \stackrel{i.i.d.}{\sim} N(\alpha_{j[i]}, \sigma^2_y),$$
$$\alpha_j|\mu_\alpha,\sigma_\alpha \stackrel{i.i.d.}{\sim} N(\mu_\alpha, \sigma^2_\alpha),$$
using default priors for $\mu_\alpha, \sigma_\alpha, \sigma_y$ as set in the brm function. In the questions below, if you present default brm-output, please indicate what brm-parameter name refers to what greek letter in the equations above.  
```{r}
# Fixed some issues with the model (increased warmup and iterations), after examining diagnostics
matri.fit <- brm(y ~ (1|ethnicgroup), data = matri, 
                 file = "output/exercise3",
                 chains = 4, iter = 2000, warmup = 1000, 
                 cores = getOption("mc.cores", 4))
```
(a) State the point estimates and 95% CIs of each of the following parameters and interpret (explain what information is given by) these estimates: $\mu_\alpha, \sigma_\alpha, \sigma_y$.  
```{r}
# Reviewing output and diagnostics
summary(matri.fit)
# names(summary(matri.fit))
summary(matri.fit)$fixed
# mu_alpha
fixef(matri.fit)
mu_alpha <- fixef(matri.fit)[1]
# sigma_alpha
summary(matri.fit)$random
# sigma_y
summary(matri.fit)$spec_pars
```
After correcting some of the settings (increasing the warmup and iterations), our diagnostics are reasonable, with Rhats of 1.00 and ESS results between 489 and 4,502. With reference to the equations above, "Intercept" refers to $\mu_\alpha$, with a mean of the group means (the grand intercept to quote Bürkner) 20.05 and a 95% CI of the posterior distribution of 19.68 - 20.46, which can be interpreted as saying that there is a 0.95 probability that the value of $\mu_\alpha$ is in that interval. Our results also show a standard deviation $\sigma_\alpha$ of 1.08, which is the standard deviation of the distribution of varying intercepts across ethnic groups. This has a CI of 0.83 - 1.41, with an interpretation similar to that of $\mu_\alpha$. The residual standard deviation, $\sigma_y$ is the error of each $y_i$ around each group mean $\mu_i$, and is 2.01, with a CI of 1.96 - 2.07. Each ethnic group has its own group mean (intercept) $\alpha_{j[i]}$ and these are:
```{r}
# eta = alpha - mu_alpha
eta <- as_tibble(ranef(matri.fit)$ethnicgroup[,,"Intercept"], rownames = "ethnicgroup")
# alpha = eta + mu_alpha
alphas <- coef(matri.fit, summary = T)$ethnicgroup %>% 
  as_tibble(rownames = "ethnicgroup") %>% 
  rename(alph = Estimate.Intercept) %>% 
  transform(ethnicgroup = as.integer(ethnicgroup))
matrigrupalpha <- alphas %>% 
  left_join(matrigrup, by = "ethnicgroup")
matrigrupalpha
```

(b) State the point estimate and 95% CIs for $\alpha_1$ and interpret (explain what information is given by) the estimate.  

This was estimated previously and is given by:
```{r}
alphas[1,]
# Interesting to compare with linear model estimate
lm.alphas.full[1,]
# They are very similar, although the CI is slightly more narrow with the linear model.
```
Thus, ethnic group 1 has a point estimate mean of 21.06, with a 95% CI of 19.32 - 22.87, which means the partially pooled (shrunk) estimate for the mean marriage age of ethnic group one is 21.06, with 95% CI given before.  

(c) Construct two plots: (a) plot of $\hat \alpha_j - \bar y_j$ against the within-ethnic-group sample size $n_j$ and (b) plot $\hat \alpha_j$ against $\bar y_j$, with the identity line added. Explain what information these plots provide regarding the comparison of $\bar y_j$ and $\hat \alpha_j$ for estimating the mean age of marriage within ethnic groups.  
```{r}
# Plot of alpha - ybar
matrigrupalpha %>% 
  ggplot(aes(y = alph - ybar, x = npers)) +
  geom_point() +
  geom_hline(yintercept = 0) +
  labs(title = "(alphahat_j - ybar_j) by group sample size", 
       x = "Group size (n_j)", 
       y = "alphahat_j - ybar_j")
# Plot of alpha ~ ybar
matrigrupalpha %>% 
  ggplot(aes(y = alph, x = ybar, size = npers)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0) + 
  geom_abline(slope = 0, intercept = mu_alpha, color = "red", ) + 
  labs(title = "alphahat_j by ybar_j with group sample size", 
       x = "Group mean (ybar_j)", 
       y = "Partially pooled mean estimates (alphahat_j)", 
       size = "Group size (n_j)") + 
  theme(legend.position = "right") + 
  annotate("text", x = 18.5, y = 20.2, 
    label = "mu_alpha", color = "red"
  )
```
The first plot is showing that for large groups (large $n_j$), the difference between the partially pooled estimates for the mean and the actual mean of each group is very small, as the weight given by the large sample size means that $\bar y_j$ and $\hat \alpha_j$ are very close. Whereas in small groups, there is great variability, and the difference between $\bar y_j$ and $\hat \alpha_j$ can be much larger as the overall group mean $\mu_\alpha$ has more weight in calculating the partially pooled mean.  

The second plot, similarly shows that in large groups $\bar y_j$ and $\hat \alpha_j$ are almost equal (seen in the proximity to the identity line), since the large sample size mean that more weight is given to $\bar y_j$ in calculating the partially pooled mean. Whereas, for smaller groups, $\mu_\alpha$ has greater weight and pulls the data in its direction, which can be seen by how they are further from the identity, but closer to $\mu_\alpha$.  


Optional/just for fun exercise: As stated in the slides, for the multilevel model under consideration
here, the full conditional distribution for the j-th state mean is given by:  

$$\alpha_j|\pmb{y}, \mu_\alpha, \sigma_y, \sigma_\alpha \sim N(m,v),$$
$$v = (n_j/ \sigma^2_y + 1/ \sigma^2_\alpha)^{-1},$$
$$m = v \cdot \left( \frac{n_j}{\sigma^2_y} \bar y + \frac{1}{\sigma^2_\alpha} \mu_\alpha \right) = \frac{\frac{n_j}{\sigma^2_y} \bar y_j + \frac{1}{\sigma^2_\alpha} \mu_\alpha}{\frac{n_j}{\sigma^2_y} + \frac{1}{\sigma^2_\alpha}}$$
Give the derivation of the full conditional.  
Hint: Check slide in part 1 for the derivation of the normal distribution for $\mu$ in the normal-normal setting.  

# Exercise 4 (4 points)
Continue with the marriage data set from exercise 3.  
The goal in this exercise is to predict the age at first marriage for a randomly sampled woman in an ethnic group for which we have not yet observed any data, using the model and data from exercise 3.  

(a) Obtain samples from the predictive posterior density for the age at first marriage and visualize the samples in a histogram. In your answer, include R code (that does NOT use the predict function from brms) as well as a write up in equations how you obtained the samples. Make sure to introduce notation first to explain what you're sampling.  

First the notation:
We assume that for our prediction, the hierarchical sampling distribution holds true, where we want to predict the age at first marriage for a randomly sampled woman $k$, in a new, unsampled ethnic group $h = j[k]$, that follows the distribution:
$$\tilde y_k|\tilde \alpha_{j[k]},\sigma_y^2 \sim N(\tilde \alpha_{j[k]}, \sigma^2_y),$$
$$\tilde \alpha_h|\mu_\alpha,\sigma^2_\alpha \sim N(\mu_\alpha, \sigma^2_\alpha),$$
with other parameters previously defined. So, I will obtain random samples for $\tilde \alpha_h$ using the parameters for $\mu_\alpha$ and $\sigma^2_\alpha$ obtained from the posterior Bayesian fit of our model. In other words, I will use the 4,000 samples of $\mu_\alpha$ and $\sigma^2_\alpha$ for each random draw of $\tilde \alpha_h$. Then, I will use those 4,000 samples of $\tilde \alpha_h$ and 4,000 samples of $\sigma^2_y$ as parameters, to obtain 4,000 random draws of $\tilde y_k$, with which I can create a histogram of my posterior predictive density.  
```{r}
# We first extract the posterior samples from our model using `as_draws` to extract 4,000 samples
# (1,000 per chain) for every parameter we're interested in.
samp <- as_draws_df(matri.fit)
# Check the size of our dataframe
dim(samp)
# Review names for later use
names(samp)[1:5]
# Make a vector of the 4,000 samples for sigma_y (residuals)
sigmay_s <- samp$sigma
# Make a vector of the samples of mu_alpha
mualpha_s <- samp$b_Intercept
# Make a vector of the samples of sigma_alpha (sd of intercepts)
sigmaalpha_s <- samp$sd_ethnicgroup__Intercept
# Length of vectors (4,000)
S <- length(sigmay_s)
# Make sampling reproducible
set.seed(1234)
# Obtain a normally distributed random sample of alphas using the sampled posterior parameters 
alphatilde_s <- rnorm(S, mualpha_s, sigmaalpha_s)
# Obtain a normally distributed random sample of ys using the sampled alphatildes and sigmays 
ytilde_s <- rnorm(S, alphatilde_s, sigmay_s)
# Obtain point estimates and 95% CI (using tidybayes):
point_interval(alphatilde_s, .point = mean)
point_interval(ytilde_s, .point = mean)
# Visualize densities
p <- as_tibble(alphatilde_s) %>% 
  ggplot(aes(alphatilde_s, after_stat(density), fill = "blue")) + 
  geom_histogram(alpha = .5, fill = "blue", bins = 60) + 
  theme_minimal() + 
  xlab("Age at first marriage") + 
  geom_vline(xintercept = mean(alphatilde_s), col = "blue")
q <- p + geom_histogram(as_tibble(ytilde_s), bins = 60, 
                        mapping = aes(ytilde_s, after_stat(density), fill = "red"), 
                        alpha = .5, fill = "red", size = 1.5) + 
  geom_vline(xintercept = mean(ytilde_s), col = "red", linetype = "dashed") 
q  + annotate("text", x = 15, y = 0.3, label = "alphatilde_s", color = "blue") + 
  annotate("text", x = 15, y = 0.25, label = "ytilde_s", color = "red")
```

(b) Use the samples to construct a point prediction and 95% prediction interval for age at first marriage. In your answer, include the expression used for calculating the point prediction from the samples.  

To calculate the point prediction from the samples, I start by using 
```{r}
# The point estimate is just the mean of the samples
(ytilde_s.mean <- mean(ytilde_s))
#For a simple approximation, I will just use the mean of sigmay_s to approximate the standard deviation
(sigmay_s.mean <- mean(sigmay_s))
# Credible intervals
(ytilde_s.ci <- c("lower" = ytilde_s.mean - (sigmay_s.mean*(qnorm(0.975))), 
                  "upper" = ytilde_s.mean + (sigmay_s.mean*(qnorm(0.975)))))
# Additionally, if I want to calculate the results for the group mean, I use:
(alphatilde_s.mean <- mean(alphatilde_s))
#For a simple approximation, I will just use the mean of sigmaalpha_s to approximate the standard deviation
(sigmaalpha_s.mean <- mean(sigmaalpha_s))
# Credible intervals
(ytilde_s.ci <- c("lower" = ytilde_s.mean - (sigmaalpha_s.mean*(qnorm(0.975))), 
                  "upper" = ytilde_s.mean + (sigmaalpha_s.mean*(qnorm(0.975)))))
```

A more exact approximation can be done using `tidybayes`:
```{r}
# Obtain point estimates and 95% CI (using tidybayes):
point_interval(ytilde_s, .width = 0.95, .point = mean)
```

(c) What is the probability that the observed age at first marriage will be greater than $\bar{y}$? In your answer, include the expression used for calculating this probability from the samples.  

The probability that the observed age at first marriage will be greater than $\bar{y}$ is given by:
$$P(\tilde y_k > \bar y) = P(\tilde y_k - \tilde \mu_\alpha >0)$$
For this, I can subtract the samples of $\tilde \mu_\alpha$ from $\tilde y_k$ and find the percentage greater than $\bar y$:
```{r}
q4.c <- ytilde_s - mualpha_s
n <- length(ytilde_s)
length(q4.c[q4.c>0])/n
# The correct answer was just:
mean(ytilde_s > ybarbar)
mean(ytilde_s > mean(mualpha_s))
```
So, the probability that the observed age at first marriage will be greater than $\bar{y}$ according to our sampled distribution is 0.496, or almost 50%.  


# Exercise 5 (extra credit, 3 points)

Obtain the joint distribution of $(y_1, y_2)|\mu_{\alpha}, \sigma_{\alpha}^2, \sigma_{y}^2$ in the setting where $j[1] \neq j[2]$ and in the setting where $j[1] = j[2]$.  

Hints:  

- First consider the univariate distribution of $y_i|\mu_{\alpha}, \sigma_{\alpha}^2, \sigma_{y}^2$.  
- You may find it helpful to write $y_i$ as the sum of parameters (thus random variables) $\mu_{\alpha}$, a parameter to capture the variability across counties, and a parameter to capture variability across observations.



